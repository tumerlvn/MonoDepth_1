{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/home/tamerlan/Masters/thesis/yolov5'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvMlHd_QwMG",
        "outputId": "e8225db4-e61d-4640-8b1f-8bfce3331cea"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv5 üöÄ 2024-4-24 Python-3.9.19 torch-2.2.2+cu121 CUDA:0 (NVIDIA GeForce GTX 1050, 4039MiB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete ‚úÖ (8 CPUs, 15.5 GB RAM, 227.3/383.9 GB disk)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import utils\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "from math import ceil\n",
        "\n",
        "display = utils.notebook_init()  # checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def colorize(\n",
        "    value: np.ndarray, vmin: float = None, vmax: float = None, cmap: str = \"afmhot\"\n",
        "):\n",
        "    if value.ndim > 2:\n",
        "        return value\n",
        "    invalid_mask = value == -1\n",
        "\n",
        "    # normalize\n",
        "    vmin = value.min() if vmin is None else vmin\n",
        "    vmax = value.max() if vmax is None else vmax\n",
        "    value = (value - vmin) / (vmax - vmin)  # vmin..vmax\n",
        "\n",
        "    # set color\n",
        "    cmapper = matplotlib.cm.get_cmap(cmap)\n",
        "    value = cmapper(value, bytes=True)  # (nxmx4)\n",
        "    value[invalid_mask] = 255\n",
        "    img = value[..., :3]\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zR9ZbuQCH7FX",
        "outputId": "284ef04b-1596-412f-88f6-948828dd2b49"
      },
      "outputs": [],
      "source": [
        "!python detect.py --weights yolov5s.pt --img 640 --conf 0.50 --source data/images\n",
        "# display.Image(filename='runs/detect/exp/zidane.jpg', width=600)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1msegment/predict: \u001b[0mweights=['yolov5m-seg.pt'], source=data/images, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.3, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=True, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/predict-seg, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1, retina_masks=False\n",
            "YOLOv5 üöÄ v7.0-304-g22361691 Python-3.9.19 torch-2.2.2+cu121 CUDA:0 (NVIDIA GeForce GTX 1050, 4039MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5m-seg summary: 301 layers, 21971597 parameters, 0 gradients, 70.8 GFLOPs\n",
            "WARNING ‚ö†Ô∏è NMS time limit 0.550s exceeded\n",
            "image 1/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00001.jpg: 384x640 3 persons, 5 cars, 1 motorcycle, 3 trucks, 225.7ms\n",
            "image 2/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00060.jpg: 384x640 5 persons, 1 bicycle, 3 cars, 1 motorcycle, 2 buss, 3 trucks, 154.1ms\n",
            "image 3/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00132.jpg: 384x640 4 persons, 2 cars, 1 motorcycle, 1 bus, 7 trucks, 141.0ms\n",
            "image 4/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00174.jpg: 384x640 3 persons, 2 cars, 4 motorcycles, 4 trucks, 126.6ms\n",
            "image 5/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00212.jpg: 384x640 3 persons, 2 cars, 6 trucks, 1 boat, 121.5ms\n",
            "image 6/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00235.jpg: 384x640 2 persons, 1 car, 1 bus, 2 trucks, 119.3ms\n",
            "image 7/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00261.jpg: 384x640 2 persons, 2 cars, 6 trucks, 118.2ms\n",
            "image 8/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00298.jpg: 384x640 8 persons, 2 cars, 2 motorcycles, 1 bus, 1 truck, 121.7ms\n",
            "image 9/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00331.jpg: 384x640 2 persons, 9 cars, 1 motorcycle, 2 buss, 3 trucks, 115.0ms\n",
            "image 10/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00382.jpg: 384x640 4 persons, 2 cars, 2 buss, 4 trucks, 140.2ms\n",
            "image 11/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00411.jpg: 384x640 1 person, 7 cars, 3 trucks, 128.3ms\n",
            "image 12/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00445.jpg: 384x640 2 persons, 5 cars, 1 motorcycle, 8 trucks, 125.1ms\n",
            "image 13/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00481.jpg: 384x640 2 persons, 3 cars, 1 bus, 6 trucks, 125.7ms\n",
            "image 14/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00495.jpg: 384x640 1 person, 2 cars, 1 motorcycle, 1 bus, 7 trucks, 124.6ms\n",
            "image 15/36 /home/tamerlan/Masters/thesis/yolov5/data/images/cam_01_00500.jpg: 384x640 1 person, 1 car, 2 buss, 7 trucks, 122.1ms\n",
            "image 16/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_0.jpg: 384x640 8 persons, 2 cars, 12 motorcycles, 1 truck, 1 umbrella, 129.3ms\n",
            "image 17/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_1.jpg: 384x640 13 persons, 1 bicycle, 6 cars, 11 motorcycles, 4 trucks, 1 umbrella, 125.0ms\n",
            "image 18/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_11.jpg: 384x640 8 persons, 1 bicycle, 1 car, 13 motorcycles, 116.2ms\n",
            "image 19/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_12.jpg: 384x640 11 persons, 1 bicycle, 1 car, 15 motorcycles, 121.6ms\n",
            "image 20/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_13.jpg: 384x640 16 persons, 1 bicycle, 1 car, 18 motorcycles, 118.6ms\n",
            "image 21/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_14.jpg: 384x640 11 persons, 6 cars, 14 motorcycles, 1 truck, 119.0ms\n",
            "image 22/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_23.jpg: 384x640 18 persons, 2 bicycles, 2 cars, 21 motorcycles, 1 truck, 115.0ms\n",
            "image 23/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_24.jpg: 384x640 15 persons, 2 bicycles, 1 car, 16 motorcycles, 1 truck, 129.2ms\n",
            "image 24/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_25.jpg: 384x640 11 persons, 1 bicycle, 4 cars, 8 motorcycles, 1 truck, 1 umbrella, 118.2ms\n",
            "image 25/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_26.jpg: 384x640 10 persons, 3 cars, 12 motorcycles, 1 truck, 1 umbrella, 129.8ms\n",
            "image 26/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_27.jpg: 384x640 4 persons, 3 motorcycles, 119.1ms\n",
            "image 27/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_3.jpg: 384x640 13 persons, 5 bicycles, 13 motorcycles, 114.6ms\n",
            "image 28/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_4.jpg: 384x640 7 persons, 1 bicycle, 8 motorcycles, 137.9ms\n",
            "image 29/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_5.jpg: 384x640 5 persons, 6 motorcycles, 125.7ms\n",
            "image 30/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_6.jpg: 384x640 2 persons, 3 motorcycles, 126.1ms\n",
            "image 31/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_7.jpg: 384x640 11 persons, 5 cars, 9 motorcycles, 3 trucks, 119.5ms\n",
            "image 32/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_72.jpg: 384x640 2 persons, 1 bus, 1 boat, 3 fire hydrants, 130.4ms\n",
            "image 33/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_73.jpg: 384x640 13 persons, 5 cars, 9 motorcycles, 3 trucks, 123.8ms\n",
            "image 34/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_74.jpg: 384x640 3 persons, 1 car, 1 motorcycle, 1 boat, 3 fire hydrants, 116.7ms\n",
            "image 35/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_75.jpg: 384x640 2 persons, 4 cars, 2 motorcycles, 126.1ms\n",
            "image 36/36 /home/tamerlan/Masters/thesis/yolov5/data/images/frame_76.jpg: 384x640 3 persons, 1 car, 1 boat, 2 fire hydrants, 114.1ms\n",
            "Speed: 1.3ms pre-process, 127.4ms inference, 67.7ms NMS per image at shape (1, 3, 640, 640)\n",
            "Results saved to \u001b[1mruns/predict-seg/exp5\u001b[0m\n",
            "36 labels saved to runs/predict-seg/exp5/labels\n"
          ]
        }
      ],
      "source": [
        "!python segment/predict.py --weights yolov5m-seg.pt --img 640 --conf 0.3 --source data/images --save-txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_pixel_coords(relative_coords, im_shape_wh):\n",
        "    return list(round(coord * dimension) for coord, dimension in zip(relative_coords, im_shape_wh))\n",
        "\n",
        "\n",
        "def get_binary_mask_of_object(segment_line, im_shape_wh, im_shape):\n",
        "    points = list(map(float, segment_line.split()))\n",
        "    points.pop(0)\n",
        "    points = np.array(points)\n",
        "    points = points.reshape(-1, 2)\n",
        "\n",
        "    pixels = []\n",
        "\n",
        "    for point in points:\n",
        "        pixels.append(to_pixel_coords(point, im_shape_wh))\n",
        "\n",
        "    pixels = np.array([pixels], dtype=np.int32)\n",
        "\n",
        "    binary_mask = np.zeros(im_shape, dtype=np.int32)\n",
        "\n",
        "    binary_mask = np.float32(cv2.fillPoly(binary_mask, pts=pixels, color=255))\n",
        "    \n",
        "    binary_mask = cv2.blur(binary_mask, (31, 31))\n",
        "    _, binary_mask = cv2.threshold(binary_mask, 254, 255, cv2.THRESH_BINARY)\n",
        "    return binary_mask\n",
        "\n",
        "def draw_object_vectors(image_path, object_segments_path, depth_pt_path, output_file):\n",
        "    im = cv2.imread(image_path)\n",
        "    im_shape = (im.shape)[0:2]\n",
        "\n",
        "\n",
        "    im_shape_wh = [im_shape[1], im_shape[0]] # w, h\n",
        "\n",
        "    with open(object_segments_path) as f:\n",
        "        lines = f.readlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            # if i == 0:\n",
        "            binary_mask = get_binary_mask_of_object(line, im_shape_wh, im_shape)\n",
        "\n",
        "            min_depth, max_depth = 1.0, 50.0\n",
        "\n",
        "            depth = torch.load(depth_pt_path, map_location=torch.device('cpu'))\n",
        "            depth = depth[0].squeeze().cpu().numpy()\n",
        "\n",
        "            depth = cv2.resize(depth,\n",
        "                    (1920,1080), # width, height\n",
        "                    interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "            \n",
        "            image_array = colorize(depth, min_depth, max_depth)\n",
        "            binary_mask_3ch = cv2.cvtColor(binary_mask, cv2.COLOR_GRAY2BGR)\n",
        "            image_array = cv2.bitwise_and(np.float32(image_array), binary_mask_3ch)\n",
        "\n",
        "            depth_of_object = cv2.bitwise_and(np.float32(depth), binary_mask)\n",
        "\n",
        "            # —É–±–∏—Ä–∞–µ–º –≤—Å–µ –≤—ã–±—Ä–æ—Å—ã\n",
        "            outliers_to_median(depth_of_object)\n",
        "\n",
        "            # –º–æ–∂–µ–º –∏—Å–∫–∞—Ç—å —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —Ç–æ—á–∫–∏ –º–∞–∫—Å–∏–º—É–º–∞ —Å—Ä–∞–∑—É, —Ç.–∫. –≤—Å–µ –∫—Ä–æ–º–µ —Ç–æ—á–µ–∫ –æ–±—ä–µ–∫—Ç–∞ - 0\n",
        "            max_point = np.unravel_index(np.argmax(depth_of_object), depth_of_object.shape) # y, x\n",
        "\n",
        "            \n",
        "            # –ø–æ–∏—Å–∫ –∏–Ω–¥–µ–∫—Å–∞ –º–∏–Ω–∏–º—É–º–∞ —Å—Ä–µ–¥–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ depth_of_object \n",
        "            # –∏–Ω–¥–µ–∫—Å—ã –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ binary_mask > 0\n",
        "            min_index = np.argmin(depth_of_object[binary_mask > 0]) \n",
        "\n",
        "            # –∏–Ω–¥–µ–∫—Å—ã –≥–¥–µ binary_mask > 0\n",
        "            indices_nonzero = np.transpose((binary_mask > 0).nonzero())\n",
        "\n",
        "            # —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —Ç–æ—á–∫–∏ –º–∏–Ω–∏–º—É–º–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏\n",
        "            min_point = indices_nonzero[min_index]\n",
        "\n",
        "            im = cv2.arrowedLine(im, (max_point[1], max_point[0]), (min_point[1], min_point[0]), \n",
        "                                                color=(0, 0, 255), thickness=3)\n",
        "            \n",
        "    cv2.imwrite(output_file, im) \n",
        "\n",
        "\n",
        "def find_close_vals(depth_of_object, val, eps=0.3):\n",
        "    result_ineq = (depth_of_object < val + eps) & (depth_of_object > val - eps) & (depth_of_object != 0)\n",
        "    indices = np.transpose(result_ineq.nonzero())\n",
        "    return indices\n",
        "\n",
        "def outliers_to_median(depth_of_object, percentile=0.01):\n",
        "    nonzero_points_mask = (depth_of_object != 0)\n",
        "    nonzero_points = depth_of_object[nonzero_points_mask]\n",
        "    nonzero_points.sort()\n",
        "\n",
        "    min_percentile_index = ceil(len(nonzero_points)*percentile) - 1\n",
        "    min_percentile = nonzero_points[min_percentile_index]\n",
        "\n",
        "    max_percentile_index = ceil(len(nonzero_points)*(1-percentile)) - 1\n",
        "    max_percentile = nonzero_points[max_percentile_index]\n",
        "\n",
        "    median_index = ceil(len(nonzero_points)*(0.5)) - 1\n",
        "    median = nonzero_points[median_index]\n",
        "\n",
        "    outliers_mask = nonzero_points_mask & ((depth_of_object <= min_percentile) | (depth_of_object >= max_percentile))\n",
        "    depth_of_object[outliers_mask] = median\n",
        "\n",
        "\n",
        "def draw_max_points(image_path, object_segments_path, depth_pt_path, output_file):\n",
        "\n",
        "    im = cv2.imread(image_path)\n",
        "    im_shape = (im.shape)[0:2]\n",
        "    im_shape_wh = [im_shape[1], im_shape[0]] # w, h\n",
        "\n",
        "\n",
        "    depth = cv2.imread('/home/tamerlan/Masters/thesis/yolov5/tmp/frame_75_depth.png', cv2.IMREAD_GRAYSCALE)\n",
        "    # depth = torch.load(depth_pt_path, map_location=torch.device('cpu'))\n",
        "    # depth = depth[0].squeeze().cpu().numpy()\n",
        "\n",
        "    # print(depth.shape)\n",
        "\n",
        "    # –∏–Ω—Ç–µ—Ä–ø–æ–ª—è—Ü–∏—è –¥–ª—è idisc\n",
        "    # depth = cv2.resize(depth,\n",
        "    #         (1920,1080), # width, height\n",
        "    #         interpolation=cv2.INTER_CUBIC)\n",
        "\n",
        "    with open(object_segments_path) as f:\n",
        "        lines = f.readlines()\n",
        "        for i, line in enumerate(lines):\n",
        "            binary_mask = get_binary_mask_of_object(line, im_shape_wh, im_shape)\n",
        "\n",
        "            \n",
        "            #############################\n",
        "            # —Å–æ–∑–¥–∞–Ω–∏–µ —Ä–∞—Å–∫—Ä–∞—à–µ–Ω–Ω–æ–π –∫–∞—Ä—Ç—ã –≥–ª—É–±–∏–Ω—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏\n",
        "            min_depth, max_depth = 1.0, 50.0\n",
        "            image_array = colorize(depth, min_depth, max_depth)\n",
        "            binary_mask_3ch = cv2.cvtColor(binary_mask, cv2.COLOR_GRAY2BGR)\n",
        "            image_array = cv2.bitwise_and(np.float32(image_array), binary_mask_3ch)\n",
        "            \n",
        "            print(depth.shape)\n",
        "            cv2.imwrite(image_array, '/home/tamerlan/Masters/thesis/yolov5/tmp/depth_check.png')\n",
        "\n",
        "            #############################\n",
        "\n",
        "            depth_of_object = cv2.bitwise_and(np.float32(depth), binary_mask)\n",
        "\n",
        "            # —É–±–∏—Ä–∞–µ–º –≤—Å–µ –≤—ã–±—Ä–æ—Å—ã\n",
        "            # outliers_to_median(depth_of_object)\n",
        "\n",
        "            # –º–æ–∂–µ–º –∏—Å–∫–∞—Ç—å —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —Ç–æ—á–∫–∏ –º–∞–∫—Å–∏–º—É–º–∞ —Å—Ä–∞–∑—É, —Ç.–∫. –≤—Å–µ –∫—Ä–æ–º–µ —Ç–æ—á–µ–∫ –æ–±—ä–µ–∫—Ç–∞ - 0\n",
        "            max_point = np.unravel_index(np.argmax(depth_of_object), depth_of_object.shape) # y, x\n",
        "            \n",
        "            # –ø–æ–∏—Å–∫ –∏–Ω–¥–µ–∫—Å–∞ –º–∏–Ω–∏–º—É–º–∞ —Å—Ä–µ–¥–∏ —ç–ª–µ–º–µ–Ω—Ç–æ–≤ depth_of_object \n",
        "            # –∏–Ω–¥–µ–∫—Å—ã –∫–æ—Ç–æ—Ä—ã—Ö —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å –∏–Ω–¥–µ–∫—Å–∞–º–∏ binary_mask > 0\n",
        "            min_index = np.argmin(depth_of_object[binary_mask > 0]) \n",
        "\n",
        "            # –∏–Ω–¥–µ–∫—Å—ã –≥–¥–µ binary_mask > 0\n",
        "            indices_nonzero = np.transpose((binary_mask > 0).nonzero())\n",
        "\n",
        "            # —Ä–∞—Å–ø–æ–ª–æ–∂–µ–Ω–∏–µ —Ç–æ—á–∫–∏ –º–∏–Ω–∏–º—É–º–∞ –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏\n",
        "            min_point = tuple(indices_nonzero[min_index])\n",
        "\n",
        "            eps = 1\n",
        "\n",
        "            max_val = depth_of_object[max_point]\n",
        "            min_val = depth_of_object[min_point]\n",
        "            \n",
        "            max_indices = np.int32(find_close_vals(depth_of_object, val = max_val, eps = eps))\n",
        "            min_indices = np.int32(find_close_vals(depth_of_object, val = min_val, eps = eps))\n",
        "            \n",
        "            overlay = im.copy()\n",
        "            overlay[max_indices[:,0],max_indices[:,1]] = (255,0,0)\n",
        "            overlay[min_indices[:,0],min_indices[:,1]] = (0,0,255)\n",
        "\n",
        "            alpha = 0.4\n",
        "            \n",
        "            im = cv2.addWeighted(overlay, alpha, im, 1 - alpha, 0) \n",
        "            \n",
        "    cv2.imwrite(output_file, im)\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "\n",
        "# binary_mask = cv2.threshold(predicted_mask, 0.5, 1, cv2.THRESH_BINARY)[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1080, 1920])\n"
          ]
        }
      ],
      "source": [
        "da1 = cv2.imread('/home/tamerlan/Masters/thesis/yolov5/tmp/frame_75_depth.png', cv2.IMREAD_GRAYSCALE)\n",
        "da1_tensor = torch.from_numpy(da1)\n",
        "print(da1_tensor.shape)\n",
        "torch.save(da1_tensor, 'da1_frame75.pt')\n",
        "\n",
        "\n",
        "# type(da1)\n",
        "\n",
        "# cv2.imwrite('check_da.png', da1) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_path = '/home/tamerlan/Masters/thesis/yolov5/data/images/frame_75.jpg'\n",
        "object_segments_path = '/home/tamerlan/Masters/thesis/yolov5/runs/predict-seg/exp5/labels/frame_75.txt'\n",
        "# depth_pt_path = '/home/tamerlan/Masters/thesis/yolov5/tmp/depth.pt'\n",
        "da1_pt_path = '/home/tamerlan/Masters/thesis/yolov5/tmp/da1_frame75.pt'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "draw_object_vectors(image_path, object_segments_path, da1_pt_path, '/home/tamerlan/Masters/thesis/yolov5/tmp/check_da.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_20492/2530438966.py:14: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed two minor releases later. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap(obj)`` instead.\n",
            "  cmapper = matplotlib.cm.get_cmap(cmap)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1080, 1920)\n",
            "(1080, 1920)\n",
            "(1080, 1920)\n",
            "(1080, 1920)\n",
            "(1080, 1920)\n",
            "(1080, 1920)\n",
            "(1080, 1920)\n",
            "(1080, 1920)\n"
          ]
        }
      ],
      "source": [
        "draw_max_points(image_path, object_segments_path, da1_pt_path, '/home/tamerlan/Masters/thesis/yolov5/tmp/check_da_maxmin.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "YOLOv5 Tutorial",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
